{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "e:\\TU\\SoSe24\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from tabulate import tabulate\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from num2words import num2words\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data exploration and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text files safely\n",
    "def read_text_file(file_path):\n",
    "    with codecs.open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = file.readlines()\n",
    "    return sentences\n",
    "\n",
    "# Function to calculate most frequent word\n",
    "def most_frequent_word(text):\n",
    "    words = text.split()\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(1)[0][0]\n",
    "\n",
    "# Function to calculate unique words count\n",
    "def count_unique_words(text):\n",
    "    words = text.split()\n",
    "    return len(set(words))\n",
    "\n",
    "# Function to calculate numeral frequencies\n",
    "def count_numerals(text):\n",
    "    numeral_count = Counter(char for char in text if char.isdigit())\n",
    "    return numeral_count\n",
    "\n",
    "# File paths\n",
    "en_file_path = 'de-en/europarl-v7.de-en.en'  # English language file\n",
    "de_file_path = 'de-en/europarl-v7.de-en.de'  # German language file\n",
    "\n",
    "# Read the files\n",
    "en_sentences = read_text_file(en_file_path)\n",
    "de_sentences = read_text_file(de_file_path)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'en': en_sentences, 'de': de_sentences})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "num_sentences = len(df)\n",
    "en_lengths = df['en'].str.len()\n",
    "de_lengths = df['de'].str.len()\n",
    "length_diff = en_lengths - de_lengths\n",
    "\n",
    "# Calculate total number of words, unique words, and average word length\n",
    "def calculate_word_stats(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    unique_words = count_unique_words(text)\n",
    "    avg_word_length = sum(len(word) for word in words) / num_words\n",
    "    return num_words, unique_words, avg_word_length\n",
    "\n",
    "en_num_words, unique_en_words, awl_en = calculate_word_stats(' '.join(df['en']))\n",
    "de_num_words, unique_de_words, awl_de = calculate_word_stats(' '.join(df['de']))\n",
    "\n",
    "# Calculate most frequent words\n",
    "most_freq_word_en = most_frequent_word(' '.join(df['en']))\n",
    "most_freq_word_de = most_frequent_word(' '.join(df['de']))\n",
    "\n",
    "# Calculate numeral frequencies\n",
    "numeral_freq_en = count_numerals(' '.join(df['en']))\n",
    "numeral_freq_de = count_numerals(' '.join(df['de']))\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = [\n",
    "    ['Number of sentences', num_sentences],\n",
    "    ['Total words (English)', en_num_words],\n",
    "    ['Total words (German)', de_num_words],\n",
    "    ['Unique words (English)', unique_en_words],\n",
    "    ['Unique words (German)', unique_de_words],\n",
    "    ['Average word length (English)', awl_en],\n",
    "    ['Average word length (German)', awl_de],\n",
    "    ['Average sentence length (English)', en_lengths.mean()],\n",
    "    ['Average sentence length (German)', de_lengths.mean()],\n",
    "    ['Average sentence length difference (English - German)', length_diff.mean()],\n",
    "    ['Most frequent word (English)', most_freq_word_en],\n",
    "    ['Most frequent word (German)', most_freq_word_de]\n",
    "    #['Numerals frequency (English)', numeral_freq_en],\n",
    "    #['Numerals frequency (German)', numeral_freq_de]\n",
    "]\n",
    "\n",
    "# Print summary statistics\n",
    "print(tabulate(summary_stats, headers=['Statistic', 'Value'], tablefmt='pretty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display word clouds\n",
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create word clouds for English and German text\n",
    "generate_word_cloud(' '.join(df['en']), 'Most Common Words in English')\n",
    "generate_word_cloud(' '.join(df['de']), 'Most Common Words in German')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['en_sentence_length'] = df['en'].str.len()\n",
    "df['de_sentence_length'] = df['de'].str.len()\n",
    "\n",
    "df['en_num_words'] = df['en'].str.split().apply(len)\n",
    "df['de_num_words'] = df['de'].str.split().apply(len)\n",
    "\n",
    "df['en_avg_word_length'] = df['en_sentence_length'] / df['en_num_words']\n",
    "df['de_avg_word_length'] = df['de_sentence_length'] / df['de_num_words']\n",
    "\n",
    "# Plotting with Seaborn\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "# Distribution of Average Word Length\n",
    "plt.subplot(1, 1, 1)\n",
    "sns.histplot(df['en_avg_word_length'], kde=True, color='blue', label='English')\n",
    "sns.histplot(df['de_avg_word_length'], kde=True, color='green', label='German')\n",
    "plt.title('Distribution of Average Word Length')\n",
    "plt.xlabel('Average Word Length (Characters)')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 10% of the data\n",
    "df_sampled = df.sample(frac=0.10, random_state=42)\n",
    "\n",
    "# Preprocessing steps\n",
    "def preprocess_text(df, column, lang, replacements):\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].apply(remove_number_lists)\n",
    "    df[column] = df[column].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "    df[column] = df[column].apply(remove_specific_characters, args=(replacements,))\n",
    "    df[column] = df[column].apply(remove_whitespace_before_numbers)\n",
    "    df[column] = df[column].apply(remove_report_number)\n",
    "    df[column] = df[column].apply(lambda x: ' '.join(x.split()))\n",
    "    df[column] = df[column].apply(convert_numbers_to_words, args=(lang,))\n",
    "    return df\n",
    "\n",
    "def remove_number_lists(text):\n",
    "    pattern = r'\\b\\d+(?:,\\s*\\d+)*\\b'\n",
    "    return re.sub(pattern, '', text).strip()\n",
    "\n",
    "def remove_specific_characters(text, replacements):\n",
    "    for key, value in replacements.items():\n",
    "        text = text.replace(key, value)\n",
    "    text = re.sub(r'[-„“‟”–…‘’´­•—‚‘υπέρœ―]', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_whitespace_before_numbers(text):\n",
    "    return re.sub(r'(\\d)\\s+(\\d)', r'\\1\\2', text)\n",
    "\n",
    "def remove_report_number(text):\n",
    "    text = re.sub(r'\\b[a-z]\\d+\\b', '', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def convert_numbers_to_words(sentence, lang='en'):\n",
    "    words = word_tokenize(sentence)\n",
    "    converted_words = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            number = int(word)\n",
    "            if 0 <= number <= 999999999:\n",
    "                converted_words.append(num2words(number, lang=lang))\n",
    "            else:\n",
    "                converted_words.append(word)\n",
    "        except (ValueError, NotImplementedError):\n",
    "            converted_words.append(word)\n",
    "    return ' '.join(converted_words)\n",
    "\n",
    "def write_files(df):\n",
    "    columns_to_save = {'en': 'en.txt', 'de': 'de.txt'}\n",
    "    for column, filename in columns_to_save.items():\n",
    "        df[column].to_csv(filename, index=False, header=False, encoding='utf-8')\n",
    "    print(\"Columns have been saved to separate files.\")\n",
    "\n",
    "# Define replacements for 'en' and 'de'\n",
    "replacements = {\n",
    "    'en': {'½': 'one half', '¾': 'three quarters', '£': 'pound', '°': 'degree', '§': 'section'},\n",
    "    'de': {'½': 'ein halb', '¾': 'drei viertel', '€': 'euro', '°': 'grad', '§': 'abschnitt'}\n",
    "}\n",
    "\n",
    "# Apply preprocessing to 'en' and 'de' columns\n",
    "df_sampled = preprocess_text(df_sampled, 'en', 'en', replacements['en'])\n",
    "df_sampled = preprocess_text(df_sampled, 'de', 'de', replacements['de'])\n",
    "\n",
    "# Strip empty lines and their correspondences\n",
    "df_sampled = df_sampled[(df_sampled['en'].str.strip() != '') & (df_sampled['de'].str.strip() != '')]\n",
    "\n",
    "# Write the processed data to files\n",
    "write_files(df_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 10 % of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  de  \\\n",
      "0  rückwürfe von bis zu zwei millionen tonnen ges...   \n",
      "1  ferner zeigt sie nützliche marktinstrumente zu...   \n",
      "2  dies bringt jedoch auch höhere kosten für die ...   \n",
      "3            bedeutende reaktionsfähigkeit potenzial   \n",
      "4  ein tätigwerden auf dem gebiet der vermarktung...   \n",
      "\n",
      "                                                  en  \n",
      "0  discarding nearly two million tonnes of health...  \n",
      "1  they can also point out useful market instrume...  \n",
      "2   however it will mean more costs for the consumer  \n",
      "3                   significant reactivity potential  \n",
      "4  the need for action on the marketing of constr...  \n"
     ]
    }
   ],
   "source": [
    "# Read and preprocess data\n",
    "with open(\"de.txt\", 'r', encoding='utf-8') as file1: \n",
    "    de_texts = [line.strip() for line in file1]\n",
    "with open(\"en.txt\", 'r', encoding='utf-8') as file2: \n",
    "    en_texts = [line.strip() for line in file2]\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df_sampled = pd.DataFrame({'de': de_texts, 'en': en_texts})\n",
    "df_sampled = df_sampled.sample(frac=0.05, random_state=1).reset_index(drop=True)\n",
    "df_sampled.tail()\n",
    "print(df_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data. 20% test and rest into train and val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (7006, 2)\n",
      "Validation data shape: (610, 2)\n",
      "Test data shape: (1904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>wir werden ab mai eine gemeinschaft mit millio...</td>\n",
       "      <td>from the first of may we shall be a community ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5944</th>\n",
       "      <td>bericht von frau maría rodríguez ramos im name...</td>\n",
       "      <td>report by maría rodríguez ramos on behalf of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9334</th>\n",
       "      <td>für das europäische projekt von morgen sind wi...</td>\n",
       "      <td>for the european project of the future we are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>schriftlich sv der bericht über die gemeinsame...</td>\n",
       "      <td>in writing sv the report on the common agricul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7075</th>\n",
       "      <td>frauen und mädchen werden unterdrückt insbeson...</td>\n",
       "      <td>women and girls suffer oppression particularly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     de  \\\n",
       "4997  wir werden ab mai eine gemeinschaft mit millio...   \n",
       "5944  bericht von frau maría rodríguez ramos im name...   \n",
       "9334  für das europäische projekt von morgen sind wi...   \n",
       "2102  schriftlich sv der bericht über die gemeinsame...   \n",
       "7075  frauen und mädchen werden unterdrückt insbeson...   \n",
       "\n",
       "                                                     en  \n",
       "4997  from the first of may we shall be a community ...  \n",
       "5944  report by maría rodríguez ramos on behalf of t...  \n",
       "9334  for the european project of the future we are ...  \n",
       "2102  in writing sv the report on the common agricul...  \n",
       "7075  women and girls suffer oppression particularly...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.2  # 20% for test set\n",
    "train_val_df, test_df = train_test_split(df_sampled, test_size=test_size, random_state=42)\n",
    "\n",
    "# Further split train/validation:\n",
    "val_size = 0.08  # 8% of remaining data for validation\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size, random_state=42)\n",
    "\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"Validation data shape:\", val_df.shape)\n",
    "print(\"Test data shape:\", test_df.shape)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DatasetDict from train_df, val_df and test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['de', 'en', '__index_level_0__'],\n",
      "        num_rows: 7006\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['de', 'en', '__index_level_0__'],\n",
      "        num_rows: 610\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['de', 'en', '__index_level_0__'],\n",
      "        num_rows: 1904\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrames to Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Create DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Print the DatasetDict\n",
    "print(dataset)\n",
    "\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "de_nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize train, val and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7006/7006 [00:00<00:00, 7881.06 examples/s]\n",
      "Map: 100%|██████████| 610/610 [00:00<00:00, 7826.99 examples/s]\n",
      "Map: 100%|██████████| 1904/1904 [00:00<00:00, 7936.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "def tokenize_example(example, max_length, sos_token, eos_token):\n",
    "    en_chars = list(example[\"en\"])[:max_length]\n",
    "    de_chars = list(example[\"de\"])[:max_length]\n",
    "\n",
    "    en_chars = [sos_token] + en_chars + [eos_token]\n",
    "    de_chars = [sos_token] + de_chars + [eos_token]\n",
    "\n",
    "    return {\"en_tokens\": en_chars, \"de_tokens\": de_chars}\n",
    "\n",
    "max_length = 1000\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"max_length\": max_length,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vocabularies with words and special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 3 # for a character to appear\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "# Tokens ONLY from train set\n",
    "all_en_tokens = train_data[\"en_tokens\"]\n",
    "all_de_tokens = train_data[\"de_tokens\"]\n",
    "\n",
    "# Build vocabularies\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    all_en_tokens,\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "de_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    all_de_tokens,\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get indices of special tokens and set return index for non existing word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<unk>', '<pad>', '<sos>', '<eos>', 'e', ' ', 'n', 'i'],\n",
       " ['<unk>', '<pad>', '<sos>', '<eos>', ' ', 'e', 't', 'o'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "en_vocab.set_default_index(unk_index)\n",
    "de_vocab.set_default_index(unk_index)\n",
    "\n",
    "x = 8\n",
    "de_vocab.get_itos()[:x], en_vocab.get_itos()[:x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7006/7006 [00:04<00:00, 1697.66 examples/s]\n",
      "Map: 100%|██████████| 610/610 [00:00<00:00, 1507.30 examples/s]\n",
      "Map: 100%|██████████| 1904/1904 [00:01<00:00, 1732.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def numericalize_example(example, en_vocab, de_vocab):\n",
    "    # Convert token lists to their corresponding indices\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n",
    "\n",
    "# Dictionary of keyword arguments for the function\n",
    "fn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n",
    "\n",
    "# Apply the numericalization function to each split of the dataset\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indices of words as torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to torch tensors\n",
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"de_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_de_ids = [example[\"de_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"de_ids\": batch_de_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders with collate_fn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will give DataLoaders based on provided dataset\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crate data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index, shuffle=False)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN based seq2seq architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=False)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(de_vocab)\n",
    "\n",
    "encoder_embedding_dim = 300\n",
    "decoder_embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        src = batch[\"en_ids\"].to(device)\n",
    "        trg = batch[\"de_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(data_loader)):\n",
    "            src = batch[\"en_ids\"].to(device)\n",
    "            trg = batch[\"de_ids\"].to(device)\n",
    "\n",
    "            output = model(src, trg, 0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embedding Matrices. Train with GloVe embeddings. Evaluate with GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"embeddings\", exist_ok=True)\n",
    "import torch\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "# Create glove embedding matrices\n",
    "def load_glove_embeddings(glove_file, vocab: Vocab, embedding_dim: int):\n",
    "    # Initialize the embedding matrix with small random values using PyTorch\n",
    "    embedding_matrix = torch.randn((len(vocab), embedding_dim)) * 0.01\n",
    "    \n",
    "    # Load pre-trained GloVe embeddings\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            if word in vocab:  # Check if the word is in the vocabulary\n",
    "                idx = vocab.get_stoi()[word]\n",
    "                embedding_vector = torch.tensor([float(val) for val in parts[1:]], dtype=torch.float32)\n",
    "                embedding_matrix[idx] = embedding_vector\n",
    "    \n",
    "    # Initialize special token embeddings\n",
    "    # 0: '<unk>', 1: '<pad>', 2: '<sos>', 3: '<eos>'\n",
    "    embedding_matrix[0] = torch.randn(embedding_dim) * 0.1  # Small random values for unknown\n",
    "    embedding_matrix[1] = torch.zeros(embedding_dim)  # Zero initialization for padding token\n",
    "    embedding_matrix[2] = torch.randn(embedding_dim) * 0.1  # Small random values (sos)\n",
    "    embedding_matrix[3] = torch.randn(embedding_dim) * 0.1  # Small random values (eos)\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_dim = 300\n",
    "en_glove_file = 'glove.6B.300d.txt'\n",
    "de_glove_file = 'glove.de.txt' # https://www.deepset.ai/german-word-embeddings\n",
    "\n",
    "char_en_gembedding_matrix = load_glove_embeddings(en_glove_file, en_vocab, embedding_dim)\n",
    "char_de_gembedding_matrix = load_glove_embeddings(de_glove_file, de_vocab, embedding_dim)\n",
    "\n",
    "# Save the embedding matrices\n",
    "# torch.save(char_en_gembedding_matrix, 'embeddings/char_en_gembedding_matrix.pt')\n",
    "# torch.save(char_de_gembedding_matrix, 'embeddings/char_de_gembedding_matrix.pt')\n",
    "\n",
    "# Load the embedding matrices\n",
    "# char_en_gembedding_matrix = torch.load('embeddings/char_en_gembedding_matrix.pt')\n",
    "# char_de_gembedding_matrix = torch.load('embeddings/char_de_gembedding_matrix.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train English to German model with GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:04<00:00,  1.13s/it]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.972 | Train PPL:  19.531\n",
      "\tValid Loss:   2.902 | Valid PPL:  18.212\n",
      "Batch 1 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:08<00:00,  1.17s/it]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.792 | Train PPL:  16.306\n",
      "\tValid Loss:   2.988 | Valid PPL:  19.844\n",
      "Batch 2 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:56<00:00,  1.60s/it]\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.674 | Train PPL:  14.502\n",
      "\tValid Loss:   2.972 | Valid PPL:  19.539\n",
      "Batch 3 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:42<00:00,  1.48s/it]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.608 | Train PPL:  13.566\n",
      "\tValid Loss:   2.989 | Valid PPL:  19.870\n",
      "Batch 4 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:34<00:00,  1.41s/it]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.564 | Train PPL:  12.985\n",
      "\tValid Loss:   2.983 | Valid PPL:  19.747\n",
      "Batch 5 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [02:08<00:00,  1.17s/it]\n",
      "100%|██████████| 10/10 [00:03<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.523 | Train PPL:  12.468\n",
      "\tValid Loss:   2.986 | Valid PPL:  19.815\n",
      "Batch 6 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:55<00:00,  1.05s/it]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.497 | Train PPL:  12.146\n",
      "\tValid Loss:   2.962 | Valid PPL:  19.338\n",
      "Batch 7 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:46<00:00,  1.03it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.469 | Train PPL:  11.809\n",
      "\tValid Loss:   2.972 | Valid PPL:  19.523\n",
      "Batch 8 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:45<00:00,  1.04it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.446 | Train PPL:  11.545\n",
      "\tValid Loss:   2.963 | Valid PPL:  19.352\n",
      "Batch 9 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:46<00:00,  1.03it/s]\n",
      "100%|██████████| 10/10 [00:02<00:00,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.426 | Train PPL:  11.317\n",
      "\tValid Loss:   2.948 | Valid PPL:  19.073\n",
      "Batch 10 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load our preptrained GloVe embeddings\n",
    "encoder.embedding.weight.data.copy_(char_en_gembedding_matrix)\n",
    "decoder.embedding.weight.data.copy_(char_de_gembedding_matrix)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"models/character_model_glove_en_de.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")\n",
    "    print(f\"Batch {epoch+1} done \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate English to German with BLEU and ROUGH scores with GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.904 | Test PPL:  18.241 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/character_model_glove_en_de.pt\"))\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = en_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = de_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == de_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = de_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1904/1904 [00:47<00:00, 39.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Function to translate sentences from English to German\n",
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"en\"],\n",
    "        model,\n",
    "        en_nlp,\n",
    "        de_nlp,\n",
    "        en_vocab,\n",
    "        de_vocab,\n",
    "        False,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the predictions for BLEU evaluation\n",
    "predictions = [\"\".join(translation[1:-1]) for translation in translations]\n",
    "references = [[example[\"de\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('de              e eeeeee',\n",
       " ['diese mission erscheint mir als besonders zweckmäßig da die europäische union der wichtigste partner sowohl in bezug auf die entwicklungshilfe als auch auf die wirtschaftliche zusammenarbeit und gleichzeitig der hauptinvestor in peru ist'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(f\"BLEU score: {results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge1': 0.0016402125124592012, 'rouge2': 0.0, 'rougeL': 0.0016173772914142618}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Adjust the function to ensure `references` is a list of single strings\n",
    "def compute_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Initialize score counters\n",
    "    scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    count = len(predictions)\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Score method expects both pred and ref to be single strings\n",
    "        score = scorer.score(ref, pred)\n",
    "        for key in scores.keys():\n",
    "            scores[key] += score[key].fmeasure\n",
    "    \n",
    "    # Average scores\n",
    "    for key in scores.keys():\n",
    "        scores[key] /= count\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Prepare predictions and references\n",
    "# Ensure references are in string format\n",
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "references = [example[\"de\"] for example in test_data]  # Ensure this is a list of strings\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = compute_rouge(predictions, references)\n",
    "print(f\"ROUGE scores: {rouge_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2vec embedding Matrices. <br>Train with Word2vec embeddings. <br>Evaluate with Word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"embeddings\", exist_ok=True)\n",
    "import gensim\n",
    "\n",
    "def load_word2vec_embeddings(word2vec_file, vocab, embedding_dim, binary=True):\n",
    "    # Initialize the embedding matrix with small random values using PyTorch\n",
    "    embedding_matrix = torch.randn((len(vocab), embedding_dim)) * 0.01\n",
    "    \n",
    "    # Get the string-to-index mapping from the vocabulary\n",
    "    stoi = vocab.get_stoi()\n",
    "    \n",
    "    if binary:\n",
    "        # Load the Word2Vec model from a binary file\n",
    "        word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "        \n",
    "        # Iterate over each word in the Word2Vec model\n",
    "        for word in word2vec.key_to_index:\n",
    "            if word in vocab:  # Check if the word is in the vocabulary\n",
    "                idx = stoi[word]  # Get the index of the word in the vocabulary\n",
    "                embedding_matrix[idx] = torch.tensor(word2vec[word])  # Place the embedding at the correct index\n",
    "    else:\n",
    "        # Read the embeddings from the text file\n",
    "        with open(word2vec_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                # Remove the b' and ' from the word part\n",
    "                word = parts[0][2:-1]\n",
    "                vector = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float)\n",
    "                if word in vocab:  # Check if the word is in the vocabulary\n",
    "                    idx = stoi[word]  # Get the index of the word in the vocabulary\n",
    "                    embedding_matrix[idx] = vector  # Place the embedding at the correct index\n",
    "\n",
    "\n",
    "    # Initialize special token embeddings\n",
    "    # 0: '<unk>', 1: '<pad>', 2: '<sos>', 3: '<eos>'\n",
    "    embedding_matrix[0] = torch.randn(embedding_dim) * 0.1  # Small random values for unknown\n",
    "    embedding_matrix[1] = torch.zeros(embedding_dim)  # Zero initialization for padding token\n",
    "    embedding_matrix[2] = torch.randn(embedding_dim) * 0.1  # Small random values (sos)\n",
    "    embedding_matrix[3] = torch.randn(embedding_dim) * 0.1  # Small random values (eos)\n",
    "    \n",
    "    return torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# Example usage\n",
    "embedding_dim = 300\n",
    "en_word2vec_file = 'GoogleNews-vectors-negative300.bin'\n",
    "de_word2vec_file = 'word2vec.de.txt'\n",
    "\n",
    "# Assuming en_vocab and de_vocab are your vocabularies of type torchtext.vocab.Vocab\n",
    "en_wembedding_matrix = load_word2vec_embeddings(en_word2vec_file, en_vocab, embedding_dim, binary=True)\n",
    "de_wembedding_matrix = load_word2vec_embeddings(de_word2vec_file, de_vocab, embedding_dim, binary=False)\n",
    "\n",
    "# Save the embedding matrices\n",
    "# torch.save(en_wembedding_matrix, 'embeddings/en_wembedding_matrix.pt')\n",
    "# torch.save(de_wembedding_matrix, 'embeddings/de_wembedding_matrix.pt')\n",
    "\n",
    "# Load the embedding matrices\n",
    "# en_wembedding_matrix = torch.load(\"embeddings/en_wembedding_matrix.pt\")\n",
    "# de_wembedding_matrix = torch.load(\"embeddings/de_wembedding_matrix.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:15<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   2.926 | Train PPL:  18.653\n",
      "\tValid Loss:   2.923 | Valid PPL:  18.588\n",
      "Batch 1 done \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6/22 [00:04<00:13,  1.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m---> 20\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate_fn(\n\u001b[0;32m     30\u001b[0m         model,\n\u001b[0;32m     31\u001b[0m         valid_data_loader,\n\u001b[0;32m     32\u001b[0m         criterion,\n\u001b[0;32m     33\u001b[0m         device,\n\u001b[0;32m     34\u001b[0m     )\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[82], line 27\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 27\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load our preptrained word2vec embeddings\n",
    "encoder.embedding.weight.data.copy_(en_wembedding_matrix)\n",
    "decoder.embedding.weight.data.copy_(de_wembedding_matrix)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "best_valid_loss = float(\"inf\")\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"models/character_model_word2vec_en_de.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")\n",
    "    print(f\"Batch {epoch+1} done \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate English to German with BLEU and ROUGH scores with Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"models/character_model_word2vec_en_de.pt\"))\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define special tokens\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "# Translate the test dataset\n",
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"en\"],\n",
    "        model,\n",
    "        en_vocab,\n",
    "        de_vocab,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the predictions and references for BLEU evaluation\n",
    "predictions = [\"\".join(translation) for translation in translations]\n",
    "references = [\"\".join(example[\"de_tokens\"][1:-1]) for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "\n",
    "print(f\"BLEU score: {results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Adjust the function to ensure `references` is a list of single strings\n",
    "def compute_rouge(predictions, references):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # Initialize score counters\n",
    "    scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    count = len(predictions)\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Score method expects both pred and ref to be single strings\n",
    "        score = scorer.score(ref, pred)\n",
    "        for key in scores.keys():\n",
    "            scores[key] += score[key].fmeasure\n",
    "    \n",
    "    # Average scores\n",
    "    for key in scores.keys():\n",
    "        scores[key] /= count\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Prepare predictions and references\n",
    "# Ensure references are in string format\n",
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "references = [example[\"de\"] for example in test_data]  # Ensure this is a list of strings\n",
    "\n",
    "# Compute ROUGE scores\n",
    "rouge_scores = compute_rouge(predictions, references)\n",
    "print(f\"ROUGE scores: {rouge_scores}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
